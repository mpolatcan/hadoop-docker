FROM ubuntu:16.04

# TODO Multi master Hadoop
# TODO Add new configurations
MAINTAINER Mutlu Polatcan <mutlupolatcan@gmail.com>

ARG HADOOP_VERSION=2.9.0
ARG HADOOP_DOWNLOAD_LINK="http://www-eu.apache.org/dist/hadoop/common/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz"

ENV JAVA_HOME "/usr/lib/jvm/java-8-openjdk-amd64"
ENV HADOOP_HOME "/usr/local/hadoop"
ENV HADOOP_CONF_DIR "/usr/local/hadoop/etc/hadoop"
ENV PATH $PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin

########################################################################################################################
############################################### HADOOP ENV VARIABLES ###################################################
########################################################################################################################
ENV HADOOP_NODE_TYPE "namenode"
ENV HADOOP_TMP_DIR "/app/hadoop/tmp"

# ----------------------- core.site.xml configs ---------------------
ENV FS_HOSTNAME "NULL"
ENV FS_DEFAULTFS_PORT "54310"
ENV FS_DEFAULTFS_PREFIX "hdfs://"
ENV FS_TRASH_INTERVAL "0"
ENV FS_TRASH_CHECKPOINT_INTERVAL "0"
ENV IO_FILE_BUFFER_SIZE "4096"
ENV IO_BYTES_PER_CHECKSUM "512"
ENV IO_SKIP_CHECKSUM_ERRORS "false"
ENV IO_MAP_INDEX_SKIP "0"
ENV IO_MAP_INDEX_INTERVAL "128"
ENV HTTP_STATICUSER_USER "dr.who"
ENV REGISTRY_RM_ENABLED "false"
ENV REGISTRY_ZK_ROOT "/registry"
ENV REGISTRY_ZK_SESSION_TIMEOUT_MS "60000"
ENV REGISTRY_ZK_CONNECTION_TIMEOUT_MS "15000"
ENV REGISTRY_ZK_RETRY_TIMES "5"
ENV REGISTRY_ZK_RETRY_INTERVAL_MS "1000"
ENV REGISTRY_ZK_RETRY_CEILING_MS "60000"
ENV REGISTRY_SECURE "false"
ENV REGISTRY_SYSTEM_ACLS "sasl:yarn@, sasl:mapred@, sasl:hdfs@"
ENV REGISTRY_ZK_QUORUM "localhost:2181"
# -------------------------------------------------------------------

# ----------------------- hdfs-site.xml configs ---------------------
ENV DFS_HOSTNAME "NULL"
ENV DFS_HTTP_IP "0.0.0.0"
ENV DFS_HTTPS_IP "0.0.0.0"
ENV DFS_NAMENODE_HTTP_PORT "50070"
ENV DFS_NAMENODE_HEARTBEAT_RECHECK_INTERVAL "300000"
ENV DFS_NAMENODE_BACKUP_PORT "50100"
ENV DFS_NAMENODE_BACKUP_HTTP_PORT "50105"
ENV DFS_NAMENODE_SECONDARY_HTTP_PORT "50090"
ENV DFS_NAMENODE_SECONDARY_HTTPS_PORT "50091"
ENV DFS_NAMENODE_REPLICATION_CONSIDERLOAD "true"
ENV DFS_NAMENODE_NAME_DIR_PREFIX "file://"
ENV DFS_NAMENODE_NAME_DIR_POSTFIX "/dfs/name"
ENV DFS_NAMENODE_DIR_RESTORE "false"
ENV DFS_NAMENODE_FS_LIMITS_MAX_COMPONENT_LENGTH "255"
ENV DFS_NAMENODE_FS_LIMITS_MAX_DIRECTORY_ITEMS "1048576"
ENV DFS_NAMENODE_FS_LIMITS_MIN_BLOCK_SIZE "1048576"
ENV DFS_NAMENODE_FS_LIMITS_MAX_BLOCKS_PER_FILE "1048576"
ENV DFS_NAMENODE_EDITS_DIR "${DFS_NAMENODE_NAME_DIR_PREFIX}${HADOOP_TMP_DIR}${DFS_NAMENODE_NAME_DIR_POSTFIX}"
ENV DFS_NAMENODE_ACLS_ENABLED "false"
ENV DFS_NAMENODE_REPLICATION_MIN "1"
ENV DFS_NAMENODE_HANDLER_COUNT "10"
ENV DFS_NAMENODE_MAX_OBJECTS "0"
ENV DFS_NAMENODE_DECOMMISSION_INTERVAL "30"
ENV DFS_NAMENODE_DECOMMISSION_BLOCKS_PER_INTERVAL "500000"
ENV DFS_NAMENODE_REPLICATION_INTERVAL "3"
ENV DFS_NAMENODE_ACCESSTIME_PRECISION "3600000"
ENV DFS_NAMENODE_CHECKPOINT_DIR_PREFIX "file://"
ENV DFS_NAMENODE_CHECKPOINT_DIR_POSTFIX "/dfs/namesecondary"
ENV DFS_NAMENODE_CHECKPOINT_EDITS_DIR "${DFS_NAMENODE_CHECKPOINT_DIR_PREFIX}${HADOOP_TMP_DIR}${DFS_NAMENODE_CHECKPOINT_DIR_POSTFIX}"
ENV DFS_NAMENODE_CHECKPOINT_PERIOD "3600"
ENV DFS_NAMENODE_CHECKPOINT_TXNS "1000000"
ENV DFS_NAMENODE_CHECKPOINT_CHECK_PERIOD "60"
ENV DFS_NAMENODE_CHECKPOINT_MAX_RETRIES "3"
ENV DFS_NAMENODE_NUM_CHECKPOINTS_RETAINED "2"
ENV DFS_DATANODE_PORT "50010"
ENV DFS_DATANODE_HTTP_PORT "50075"
ENV DFS_DATANODE_HTTPS_PORT "50475"
ENV DFS_DATANODE_HANDLER_COUNT "10"
ENV DFS_DATANODE_DU_RESERVED "0"
ENV DFS_DATANODE_DATA_DIR_PREFIX "file://"
ENV DFS_DATANODE_DATA_DIR_POSTFIX "/dfs/data"
ENV DFS_DATANODE_DATA_DIR_PERM "700"
ENV DFS_DATANODE_DIRECTORYSCAN_INTERVAL "21600"
ENV DFS_DATANODE_DIRECTORYSCAN_THREADS "1"
ENV DFS_DATANODE_MAX_TRANSFER_THREADS "4096"
ENV DFS_DATANODE_SCAN_PERIOD_HOURS "504"
ENV DFS_DATANODE_READAHEAD_BYTES "4194304"
ENV DFS_DATANODE_DROP_CACHE_BEHIND_WRITES "false"
ENV DFS_DATANODE_SYNC_BEHIND_WRITES "false"
ENV DFS_DATANODE_USE_DATANODE_HOSTNAME "false"
ENV DFS_DATANODE_HDFS_BLOCKS_METADATA_ENABLED "false"
ENV DFS_HTTP_POLICY "HTTP_ONLY"
ENV DFS_CLIENT_HTTPS_NEED_AUTH "false"
ENV DFS_CLIENT_CACHED_CONN_RETRY "3"
ENV DFS_BLOCKSIZE "134217728"
ENV DFS_PERMISSIONS_ENABLED "false"
ENV DFS_PERMISSIONS_SUPERUSERGROUP "hadoop"
ENV DFS_REPLICATION "1"
ENV DFS_REPLICATION_MAX "512"
ENV DFS_HEARTBEAT_INTERVAL "3"
ENV DFS_CLIENT_BLOCK_WRITE_RETRIES "3"
ENV DFS_CLIENT_BLOCK_WRITE_REPLACE_DATANODE_ON_FAILURE_ENABLE "true"
ENV DFS_CLIENT_BLOCK_WRITE_REPLACE_DATANODE_ON_FAILURE_POLICY "DEFAULT"
ENV DFS_CLIENT_BLOCK_WRITE_REPLACE_DATANODE_ON_FAILURE_BEST_EFFORT "false"
ENV DFS_BLOCKREPORT_INTERVAL_MS "21600000"
ENV DFS_BLOCKREPORT_INITIAL_DELAY "0"
ENV DFS_BLOCKREPORT_SPLIT_THRESHOLD "1000000"
ENV DFS_STREAM_BUFFER_SIZE "4096"
ENV DFS_BYTES_PER_CHECKSUM "512"
ENV DFS_CLIENT_WRITE_PACKET_SIZE "65536"
ENV DFS_IMAGE_COMPRESS "false"
# -------------------------------------------------------------------

# ---------------------- mapred-site.xml configs ---------------------
ENV MAPREDUCE_HOSTNAME "NULL"
ENV MAPREDUCE_FRAMEWORK_NAME "yarn"
ENV MAPREDUCE_MAP_MEMORY_MB "1024"
ENV MAPREDUCE_MAP_CPU_VCORES "1"
ENV MAPREDUCE_REDUCE_MEMORY_MB "1024"
ENV MAPREDUCE_REDUCE_CPU_VCORES "1"
ENV MAPREDUCE_JOBTRACKER_ADDR "local"
ENV MAPREDUCE_JOBHISTORY_WEBAPP_IP "0.0.0.0"
ENV MAPREDUCE_JOBTRACKER_HTTP_PORT "50030"
ENV MAPREDUCE_TASKTRACKER_HTTP_PORT "50060"
ENV MAPREDUCE_JOB_USERLOG_RETAIN_HOURS "24"
ENV MAPREDUCE_JOBTRACKER_HANDLER_COUNT "10"
ENV MAPREDUCE_JOB_MAPS "2"
ENV MAPREDUCE_JOB_REDUCES "1"
ENV MAPREDUCE_JOBTRACKER_JOBHISTORY_BLOCK_SIZE "3145728"
ENV MAPREDUCE_JOB_RUNNING_MAP_LIMIT "0"
ENV MAPREDUCE_JOB_RUNNING_REDUCE_LIMIT "0"
ENV MAPREDUCE_TASKTRACKER_MAP_TASKS_MAXIMUM "2"
ENV MAPREDUCE_TASKTRACKER_REDUCE_TASKS_MAXIMUM "2"
ENV MAPREDUCE_TASK_IO_SORT_FACTOR "10"
ENV MAPREDUCE_TASK_IO_SORT_MB "100"
ENV MAPREDUCE_REDUCE_SHUFFLE_PARALLELCOPIES "5"
ENV MAPREDUCE_JOBHISTORY_PORT "10020"
ENV MAPREDUCE_JOBHISTORY_WEBAPP_PORT "1988"
# -------------------------------------------------------------------

# ---------------------- yarn-site.xml configs -----------------------
ENV YARN_RM_HOSTNAME "NULL"
ENV YARN_RM_WEBAPP_IP "0.0.0.0"
ENV YARN_RM_PORT "8032"
ENV YARN_RM_CLIENT_THREAD_COUNT "50"
ENV YARN_RM_SCHEDULER_PORT "8030"
ENV YARN_RM_SCHEDULER_CLIENT_THREAD_COUNT "50"
ENV YARN_HTTP_POLICY "HTTP_ONLY"
ENV YARN_RM_WEBAPP_PORT "8088"
ENV YARN_RM_WEBAPP_HTTPS_PORT "8090"
ENV YARN_RM_TRACKER_PORT "8031"
ENV YARN_ACL_ENABLE "false"
ENV YARN_ADMIN_ACL "*"
ENV YARN_RM_ADMIN_PORT "8033"
ENV YARN_RM_ADMIN_CLIENT_THREAD_COUNT "1"
ENV YARN_RM_CONNECT_MAX_WAIT_MS "900000"
ENV YARN_RM_CONNECT_RETRY_INTERVAL_MS "30000"
ENV YARN_RM_SCHEDULER_MIN_ALLOC_MB "1024"
ENV YARN_RM_SCHEDULER_MAX_ALLOC_MB "8192"
ENV YARN_RM_SCHEDULER_MIN_ALLOC_VCORES "1"
ENV YARN_RM_SCHEDULER_MAX_ALLOC_VCORES "32"
ENV YARN_RM_RECOVERY_ENABLED "false"
ENV YARN_RM_STORE_CLASS "FileSystem"
ENV YARN_RM_STATE_STORE_FILESYSTEM "org.apache.hadoop.yarn.server.resourcemanager.recovery.FileSystemRMStateStore"
ENV YARN_RM_STATE_STORE_ZOOKEEPER "org.apache.hadoop.yarn.server.resourcemanager.recovery.ZKRMStateStore"
ENV YARN_RM_MAX_COMPLETED_APPS "10000"
ENV YARN_RM_STATE_STORE_MAX_COMPLETED_APPS "${YARN_RM_MAX_COMPLETED_APPS}"
ENV YARN_RM_ZK_ADDR "NULL"
ENV YARN_RM_ZK_NUM_RETRIES "500"
ENV YARN_RM_ZK_RETRY_INTERVAL_MS "2000"
ENV YARN_RM_ZK_STATE_STORE_PARENT_PATH "/rmstore"
ENV YARN_RM_ZK_TIMEOUT_MS "10000"
ENV YARN_RM_ZK_ACL "world:anyone:rwcda"
ENV YARN_RM_ZK_STATE_STORE_ROOT_NODE_ACL "NULL"
ENV YARN_RM_FS_STATE_STORE_URI_POSTFIX "/yarn/system/rmstore"
ENV YARN_RM_FS_STATE_STORE_RETRY_POLICY_SPEC "2000,500"
ENV YARN_RM_HA_ENABLED "false"
ENV YARN_RM_HA_AUTOMATIC_FAILOVER_ENABLED "true"
ENV YARN_RM_HA_AUTOMATIC_FAILOVER_EMBEDDED "true"
ENV YARN_RM_HA_AUTOMATIC_FAILOVER_ZK_BASE_PATH "/yarn-leader-election"
ENV YARN_RM_CLUSTER_ID "NULL"
ENV YARN_RM_HA_RM_IDS "NULL"
ENV YARN_RM_HA_ID "NULL"
ENV YARN_CLIENT_FAILOVER_PROXY_PROVIDER "org.apache.hadoop.yarn.client.ConfiguredRMFailoverProxyProvider"
ENV YARN_CLIENT_FAILOVER_MAX_ATTEMPTS "NULL"
ENV YARN_CLIENT_FAILOVER_SLEEP_BASE_MS "NULL"
ENV YARN_CLIENT_FAILOVER_SLEEP_MAX_MS "NULL"
ENV YARN_CLIENT_FAILOVER_RETRIES "0"
ENV YARN_CLIENT_FAILOVER_RETRIES_ON_SOCKET_TIMEOUTS "NULL"
ENV YARN_RM_NODEMANAGERS_HEARTBEAT_INTERVAL_MS "1000"
ENV YARN_RM_SCHEDULER_MONITOR_ENABLE "false"
ENV YARN_RM_SCHEDULER_MONITOR_POLICIES "org.apache.hadoop.yarn.server.resourcemanager.monitor.capacity.ProportionalCapacityPreemptionPolicy"
ENV YARN_NM_HOSTNAME "NULL"
ENV YARN_NM_PORT "5050"
ENV YARN_NM_DELETE_THREAD_COUNT "4"
ENV YARN_NM_DELETE_DEBUG_DELAY_SEC "0"
ENV YARN_NM_LOCAL_DIRS_POSTFIX "/nm-local-dir"
ENV YARN_NM_LOCAL_CACHE_MAX_FILES_PER_DIR "8192"
ENV YARN_NM_LOCALIZER_PORT "8040"
ENV YARN_NM_LOCALIZER_CACHE_CLEANUP_INTERVAL_MS "600000"
ENV YARN_NM_LOCALIZER_CACHE_TARGET_SIZE_MB "10240"
ENV YARN_NM_LOCALIZER_CLIENT_THREAD_COUNT "5"
ENV YARN_NM_LOCALIZER_FETCH_THREAD_COUNT "4"
ENV YARN_LOG_AGGREGATION_ENABLED "false"
ENV YARN_LOG_AGGREGATION_RETAIN_SECONDS "-1"
ENV YARN_LOG_AGGREGATION_RETAIN_CHECK_INTERVAL_SECONDS "-1"
ENV YARN_NM_LOG_RETAIN_SECONDS "10800"
ENV YARN_NM_WEBAPP_PORT "8042"
ENV YARN_NM_LOG_AGGREGATION_COMPRESSION_TYPE "none"
ENV YARN_NM_AUX_SERVICES "mapreduce_shuffle"
ENV YARN_TIMELINE_SERVICE_ENABLED "false"
ENV YARN_TIMELINE_SERVICE_HOSTNAME "0.0.0.0"
ENV YARN_TIMELINE_SERVICE_PORT "10200"
ENV YARN_TIMELINE_SERVICE_WEBAPP_PORT "8188"
ENV YARN_TIMELINE_SERVICE_WEBAPP_HTTPS_PORT "8190"
ENV YARN_TIMELINE_SERVICE_STORE_CLASS "org.apache.hadoop.yarn.server.applicationhistoryservice.timeline.LeveldbTimelineStore"
ENV YARN_TIMELINE_SERVICE_TTL_ENABLED "true"
ENV YARN_TIMELINE_SERVICE_TTL_MS "604800000"
ENV YARN_TIMELINE_SERVICE_LEVELDB_TIMELINE_STORE_PATH_POSTFIX "/yarn/timeline"
ENV YARN_TIMELINE_SERVICE_LEVELDB_TIMELINE_STORE_TTL_INTERVAL_MS "300000"
ENV YARN_TIMELINE_SERVICE_LEVELDB_TIMELINE_STORE_READ_CACHE_SIZE "104857600"
ENV YARN_TIMELINE_SERVICE_LEVELDB_TIMELINE_STORE_START_TIME_READ_CACHE_SIZE "10000"
ENV YARN_TIMELINE_SERVICE_LEVELDB_TIMELINE_STORE_START_TIME_WRITE_CACHE_SIZE "10000"
ENV YARN_TIMELINE_SERVICE_HANDLER_THREAD_COUNT "10"
########################################################################################################################

COPY entrypoint.sh hadoop_config_loader.sh ./

# 1 - Update and Install Required Packages
RUN apt-get update && \
    apt-get -y install --no-install-recommends nano htop default-jre default-jdk wget && \
    rm -rf /var/lib/apt/lists/* && \
    # 2 - Add Hadoop user to system
    addgroup hadoop && adduser --disabled-password --gecos "" --ingroup hadoop hduser && \
    # 3 - Extract and Install Hadoop
    mkdir -p ${HADOOP_HOME} && \
    wget ${HADOOP_DOWNLOAD_LINK} && \
    tar -xvzf hadoop-${HADOOP_VERSION}.tar.gz -C /usr/local && \
    mv /usr/local/hadoop-${HADOOP_VERSION}/* ${HADOOP_HOME} && \
    rm -r /usr/local/hadoop-${HADOOP_VERSION} && \
    rm hadoop-${HADOOP_VERSION}.tar.gz && \
    chown -R hduser:hadoop ${HADOOP_HOME} && \
    chmod -R 777 ${HADOOP_HOME} && \
    # 4 - Create tmp directory for Hadoop
    mkdir -p ${HADOOP_TMP_DIR} && \
    chown -R hduser:hadoop ${HADOOP_TMP_DIR} && \
    chmod -R 777 ${HADOOP_TMP_DIR} && \
    # 5 - Make scripts executable
    chmod +x entrypoint.sh hadoop_config_loader.sh

ENTRYPOINT ["./entrypoint.sh"]