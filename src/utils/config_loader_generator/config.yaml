config_loader_sh_template: |
  #!/usr/bin/env bash

  function load_config() {{
      if [[ "$2" != "NULL" ]]
          then
              printf "\t<property>\n\t\t<name>$1</name>\n\t\t<value>$2</value>\n\t</property>\n" >> "${{HADOOP_CONF_DIR}}/$3"
      fi
  }}

  function load_config_with_opt() {{
      if [[ "$2" != "NULL" ]]
          then
              printf "\t<property>\n\t\t<name>$1</name>\n\t\t<value>$3</value>\n\t</property>\n" >> "${{HADOOP_CONF_DIR}}/$5"
      else
          printf "\t<property>\n\t\t<name>$1</name>\n\t\t<value>$4</value>\n\t</property>\n" >> "${{HADOOP_CONF_DIR}}/$5"
      fi
  }}

  {begin_load_fn_calls}

  # Load ResourceManager (YARN) HA configurations
  if [[ "${{YARN_RESOURCEMANAGER_HA_ENABLED}}" == "true" ]]; then
      IFS="," read -r -a RESOURCEMANAGER_IDS <<< ${{YARN_RESOURCEMANAGER_HA_RM_IDS}}

      for RESOURCEMANAGER_ID in ${{RESOURCEMANAGER_IDS[@]}}; do
          ENV_VAR_SUFFIX=$(echo $RESOURCEMANAGER_ID | tr '[a-z]' '[A-Z]')
          ENV_VAR_YARN_RESOURCEMANAGER_HOSTNAME="YARN_RESOURCEMANAGER_HOSTNAME_${{ENV_VAR_SUFFIX}}"
          ENV_VAR_YARN_RESOURCEMANAGER_ADDRESS="YARN_RESOURCEMANAGER_ADDRESS_${{ENV_VAR_SUFFIX}}"
          ENV_VAR_YARN_RESOURCEMANAGER_SCHEDULER_ADDRESS="YARN_RESOURCEMANAGER_SCHEDULER_ADDRESS_${{ENV_VAR_SUFFIX}}"
          ENV_VAR_YARN_RESOURCEMANAGER_RESOURCE_TRACKER_ADDRESS="YARN_RESOURCEMANAGER_RESOURCE_TRACKER_ADDRESS_${{ENV_VAR_SUFFIX}}"
          ENV_VAR_YARN_RESOURCEMANAGER_ADMIN_ADDRESS="YARN_RESOURCEMANAGER_ADMIN_ADDRESS_${{ENV_VAR_SUFFIX}}"
          ENV_VAR_YARN_RESOURCEMANAGER_WEBAPP_ADDRESS="YARN_RESOURCEMANAGER_WEBAPP_ADDRESS_${{ENV_VAR_SUFFIX}}"
          ENV_VAR_YARN_RESOURCEMANAGER_WEBAPP_HTTPS_ADDRESS="YARN_RESOURCEMANAGER_WEBAPP_HTTPS_ADDRESS_${{ENV_VAR_SUFFIX}}"

          load_config "yarn.resourcemanager.hostname.$RESOURCEMANAGER_ID" "${{!ENV_VAR_YARN_RESOURCEMANAGER_HOSTNAME:=NULL}}" "yarn-site.xml"
          load_config "yarn.resourcemanager.address.$RESOURCEMANAGER_ID" "${{!ENV_VAR_YARN_RESOURCEMANAGER_ADDRESS:=NULL}}" "yarn-site.xml"
          load_config "yarn.resourcemanager.scheduler.address.$RESOURCEMANAGER_ID" "${{!ENV_VAR_YARN_RESOURCEMANAGER_SCHEDULER_ADDRESS:=NULL}}" "yarn-site.xml"
          load_config "yarn.resourcemanager.resource-tracker.address.$RESOURCEMANAGER_ID" "${{!ENV_VAR_YARN_RESOURCEMANAGER_RESOURCE_TRACKER_ADDRESS:=NULL}}" "yarn-site.xml"
          load_config "yarn.resourcemanager.admin.address.$RESOURCEMANAGER_ID" "${{!ENV_VAR_YARN_RESOURCEMANAGER_ADMIN_ADDRESS:=NULL}}" "yarn-site.xml"
          load_config "yarn.resourcemanager.webapp.address.$RESOURCEMANAGER_ID" "${{!ENV_VAR_YARN_RESOURCEMANAGER_WEBAPP_ADDRESS:=NULL}}" "yarn-site.xml"
          load_config "yarn.resourcemanager.webapp.https.address.$RESOURCEMANAGER_ID" "${{!ENV_VAR_YARN_RESOURCEMANAGER_WEBAPP_HTTPS_ADDRESS:=NULL}}" "yarn-site.xml"
      done
  fi

  # ====================================================================================================================

  # Load HDFS HA configurations
  if [[ "${{DFS_NAMESERVICES:=NULL}}" != "NULL" ]]; then
      IFS="," read -r -a DFS_NAMESERVICE_IDS <<< ${{DFS_NAMESERVICES}}

      for DFS_NAMESERVICE_ID in ${{DFS_NAMESERVICES[@]}}; do
          ENV_VAR_SUFFIX_NAMESERVICE_ID=$(echo $DFS_NAMESERVICE_ID | tr '[a-z]' '[A-Z]')
          ENV_VAR_DFS_HA_NAMENODES="DFS_HA_NAMENODES_${{ENV_VAR_SUFFIX_NAMESERVICE_ID}}"
          ENV_VAR_DFS_CLIENT_FAILOVER_PROXY_PROVIDER="DFS_CLIENT_FAILOVER_PROXY_PROVIDER_${{ENV_VAR_SUFFIX_NAMESERVICE_ID}}"

          load_config "dfs.ha.namenodes.$DFS_NAMESERVICE_ID" "${{!ENV_VAR_DFS_HA_NAMENODES:=NULL}}" "hdfs-site.xml"
          load_config "dfs.client.failover.proxy.provider.$DFS_NAMESERVICE_ID" "${{!ENV_VAR_DFS_CLIENT_FAILOVER_PROXY_PROVIDER:=org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider}}" "hdfs-site.xml"

          # Load HDFS HA Namenode configurations
          IFS="," read -r -a DFS_HA_NAMENODE_IDS <<< ${{!ENV_VAR_DFS_HA_NAMENODES:=NULL}}

          for DFS_HA_NAMENODE_ID in ${{DFS_HA_NAMENODE_IDS[@]}}; do
              ENV_VAR_SUFFIX_DFS_HA_NAMENODE_ID=$(echo $DFS_HA_NAMENODE_ID | tr '[a-z]' '[A-Z]')
              ENV_VAR_DFS_NAMENODE_RPC_ADDRESS="DFS_NAMENODE_RPC_ADDRESS_${{ENV_VAR_SUFFIX_NAMESERVICE_ID}}_${{ENV_VAR_SUFFIX_DFS_HA_NAMENODE_ID}}"
              ENV_VAR_DFS_NAMENODE_HTTP_ADDRESS="DFS_NAMENODE_HTTP_ADDRESS_${{ENV_VAR_SUFFIX_NAMESERVICE_ID}}_${{ENV_VAR_SUFFIX_DFS_HA_NAMENODE_ID}}"

              load_config "dfs.namenode.rpc-address.$DFS_NAMESERVICE_ID.$DFS_HA_NAMENODE_ID" "${{!ENV_VAR_DFS_NAMENODE_RPC_ADDRESS:=NULL}}" "hdfs-site.xml"
              load_config "dfs.namenode.http-address.$DFS_NAMESERVICE_ID.$DFS_HA_NAMENODE_ID" "${{!ENV_VAR_DFS_NAMENODE_HTTP_ADDRESS:=NULL}}" "hdfs-site.xml"
          done
      done
  fi

  # Add additional JARs to Hadoop classpath like AWS, S3 and Hadoop Streaming
  echo "export HADOOP_CLASSPATH=\$HADOOP_CLASSPATH:\$HADOOP_HOME/share/hadoop/tools/lib/*" >> "${{HADOOP_HOME}}/etc/hadoop/hadoop-env.sh"

  {end_load_fn_calls}

config_files:
  - path: hadoop_configs/hdfs-site.yaml
    filename: hdfs-site.xml
  - path: hadoop_configs/yarn-site.yaml
    filename: yarn-site.xml
  - path: hadoop_configs/core-site.yaml
    filename: core-site.xml
  - path: hadoop_configs/mapred-site.yaml
    filename: mapred-site.xml

output_dir: ../../base