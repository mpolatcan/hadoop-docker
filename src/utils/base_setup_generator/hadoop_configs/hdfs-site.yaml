dfs:
  namenode:
    hostname: "NULL"
    rpc-address:
      "": "?${DFS_NAMENODE_HOSTNAME},${DFS_NAMENODE_HOSTNAME}:${DFS_NAMENODE_RPC_PORT},${HOSTNAME}:${DFS_NAMENODE_RPC_PORT}"
      auxiliary-ports: "NULL"
    rpc-port: "8020"
    rpc-bind-host: "0.0.0.0"
    servicerpc-address: "NULL"
    servicerpc-bind-host: "0.0.0.0"
    lifeline:
      rpc-address: "NULL"
      rpc-bind-host: "NULL"
      handler:
        ratio: "0.10"
        count: "NULL"
    secondary:
      hostname: "NULL"
      http-port: "9868"
      http-address: "?${DFS_NAMENODE_SECONDARY_HOSTNAME},${DFS_NAMENODE_SECONDARY_HOSTNAME}:${DFS_NAMENODE_SECONDARY_HTTP_PORT},${HOSTNAME}:${DFS_NAMENODE_SECONDARY_HTTP_PORT}"
      https-port: "9869"
      https-address: "?${DFS_NAMENODE_SECONDARY_HOSTNAME},${DFS_NAMENODE_SECONDARY_HOSTNAME}:${DFS_NAMENODE_SECONDARY_HTTPS_PORT},${HOSTNAME}:${DFS_NAMENODE_SECONDARY_HTTPS_PORT}"
    http-port: "9870"
    https-port: "9871"
    http-address: "?${DFS_NAMENODE_HOSTNAME},${DFS_NAMENODE_HOSTNAME}:${DFS_NAMENODE_HTTP_PORT},${HOSTNAME}:${DFS_NAMENODE_HTTP_PORT}"
    https-address: "?${DFS_NAMENODE_HOSTNAME},${DFS_NAMENODE_HOSTNAME}:${DFS_NAMENODE_HTTPS_PORT},${HOSTNAME}:${DFS_NAMENODE_HTTPS_PORT}"
    http-bind-host: "0.0.0.0"
    https-bind-host: "0.0.0.0"
    heartbeat:
      recheck-interval: "300000"
    backup:
      hostname: "NULL"
      port: "50100"
      address: "?${DFS_NAMENODE_BACKUP_HOSTNAME},${DFS_NAMENODE_BACKUP_HOSTNAME}:${DFS_NAMENODE_BACKUP_PORT},${HOSTNAME}:${DFS_NAMENODE_BACKUP_PORT}"
      http-port: "50105"
      http-address: "?${DFS_NAMENODE_BACKUP_HOSTNAME},${DFS_NAMENODE_BACKUP_HOSTNAME}:${DFS_NAMENODE_BACKUP_HTTP_PORT},${HOSTNAME}:${DFS_NAMENODE_BACKUP_HTTP_PORT}"
      dnrpc-address: "NULL"
    redundancy:
      considerLoad:
        "": "true"
        factor: "2.0"
      interval:
        seconds: "3s"
    name:
      dir:
        "": "=file://${HADOOP_TMP_DIR}/dfs/name"
        restore: "false"
      cache:
        threshold: "10"
    fs-limits:
      max-component-length: "255"
      max-directory-items: "1048576"
      min-block-size: "1048576"
      max-blocks-per-file: "10000"
      max-xattrs-per-inode: "32"
      max-xattr-size: "16384"
    edits:
      dir:
        "": "=${DFS_NAMENODE_NAME_DIR}"
        required: "NULL"
        minimum: "1"
      journal-plugin:
        "": "NULL"
        qjournal: "org.apache.hadoop.hdfs.qjournal.client.QuorumJournalManager"
      noeditlogchannelflush: "false"
      asynclogging: "true"
    shared:
      edits:
        dir: "NULL"
    acls:
      enabled: "false"
    posix:
      acl:
        inheritance:
          enabled: "true"
    lazypersist:
      file:
        scrub:
          interval:
            sec: "300"
    replication:
      min: "1"
      max-streams: "2"
      max-streams-hard-limit: "4"
      work:
        multiplier:
          per:
            iteration: "2"
    maintenance:
      replication:
        min: "1"
    safemode:
      replication:
        min: "NULL"
      threshold-pct: "0.999f"
      min:
        datanodes: "0"
      extension: "30000"
    max-corrupt-file-blocks-returned: "100"
    max:
      full:
        block:
          report:
            leases: "6"
      objects: "0"
      extra:
        edits:
          segments:
            retained: "10000"
      op:
        size: "52428800"
    full:
      block:
        report:
          lease:
            length:
              ms: "300000"
    handler:
      count: "10"
    service:
      handler:
        count: "10"
    resource:
      check:
        interval: "5000"
      du:
        reserved: "104857600"
      checked:
        volumes:
          "": "NULL"
          minimum: "1"
    datanode:
      registration:
        ip-hostname-check: "true"
    decomission:
      interval: "30s"
      blocks:
        per:
          interval: "500000"
      max:
        concurrent:
          tracked:
            nodes: "100"
    accesstime:
      precision: "3600000"
    plugins: "NULL"
    block-placement-policy:
      default:
        prefer-local-node: "true"
    checkpoint:
      dir: "=file://${HADOOP_TMP_DIR}/dfs/namesecondary"
      edits:
        dir: "=${DFS_NAMENODE_CHECKPOINT_DIR}"
      period: "3600s"
      txns: "1000000"
      check:
        period: "60s"
        quiet-multiplier: "1.5"
      max-retries: "3"
    num:
      checkpoints:
        retained: "2"
      extra:
        edits:
          retained: "1000000"
    delegation:
      key:
        update-interval: "86400000"
      token:
        max-lifetime: "604800000"
        renew-interval: "86400000"
        always-use: "false"
    support:
      allow:
        format: "true"
    kerberos:
      principal:
        "": "NULL"
        pattern: "*"
      internal:
        spnego:
          principal: "=${DFS_WEB_AUTHENTICATION_KERBEROS_PRINCIPAL}"
    keytab:
      file: "NULL"
    avoid:
      read:
        stale:
          datanode: "false"
      write:
        stale:
          datanode: "false"
    stale:
      datanode:
        minimum:
          interval: "3"
        interval: "30000"
    write:
      stale:
        datanode:
          ratio: "0.5f"
    invalidate:
      work:
        pct:
          per:
            iteration: "0.32f"
    metrics:
      logger:
        period:
          seconds: "600"
    audit:
      loggers: "default"
      log:
        debug:
          cmdlist: "NULL"
        async: "false"
        token:
          tracking:
            id: "false"
    enable:
      retrycache: "true"
    retrycache:
      expirytime:
        millis: "600000"
      heap:
        percent: "0.03f"
    caching:
      enabled: "true"
    path:
      based:
        cache:
          block:
            map:
              allocation:
                percent: "0.25"
          refresh:
            interval:
              ms: "30000"
          retry:
            interval:
              ms: "30000"
    list:
      cache:
        directives:
          num:
            responses: "100"
        pools:
          num:
            responses: "100"
      encryption:
        zones:
          num:
            responses: "100"
      reencryption:
        status:
          num:
            responses: "100"
      openfiles:
        num:
          responses: "1000"
    edit:
      log:
        autoroll:
          multiplier:
            threshold: "0.5"
          check:
            interval:
              ms: "300000"
    reject-unresolved-dn-topology-mapping: "false"
    xattrs:
      enabled: "true"
    lease-recheck-interval-ms: "2000"
    max-lock-hold-to-release-lease-ms: "25"
    write-lock-reporting-threshold-ms: "5000"
    read-lock-reporting-threshold-ms: "5000"
    lock:
      detailed-metrics:
        enabled: "false"
    fslock:
      fair: "true"
    startup:
      delay:
        block:
          deletion:
            sec: "0"
    edekcacheloader:
      interval:
        ms: "1000"
      initial:
        delay:
          ms: "3000"
    reencrypt:
      sleep:
        interval: "1m"
      batch:
        size: "1000"
      throttle:
        limit:
          handler:
            ratio: "1.0"
          updater:
            ratio: "1.0"
      edek:
        threads: "10"
    inotify:
      max:
        events:
          per:
            rpc: "1000"
    legacy-oiv-image:
      dir: "NULL"
    top:
      enabled: "true"
      window:
        num:
          buckets: "10"
      num:
        users: "10"
      windows:
        minutes: "1,5,25"
    blocks:
      per:
        postponedblocks:
          rescan: "10000"
    ec:
      system:
        default:
          policy: "RS-6-3-1024k"
      policies:
        max:
          cellsize: "4194304"
    quota:
      init-threads: "4"
    upgrade:
      domain:
        factor: "=${DFS_REPLICATION}"
    hosts:
      provider:
        classname: "org.apache.hadoop.hdfs.server.blockmanagement.HostFileManager"
    available-space-block-placement-policy:
      balanced-space-preference-fraction: "0.6"
    file:
      close:
        num-committed-allowed: "0"
    inode:
      attributes:
        provider:
          class: "NULL"
          bypass:
            users: "NULL"
    max-num-blocks-to-log: "1000"
    missing:
      checkpoint:
        periods:
          before:
            shutdown: "3"
    reconstruction:
      pending:
        timeout-sec: "300"
    storageinfo:
      defragment:
        timeout:
          ms: "4"
        interval:
          ms: "600000"
        ratio: "0.75"
    snapshot:
      capture:
        openfiles: "false"
      skip:
        capture:
          accesstime-only-change: "false"
      max:
        limit: "65536"
      skiplist:
        max:
          levels: "0"
        interval: "10"
    snapshotdiff:
      allow:
        snap-root-descendant: "true"
      listing:
        limit: "1000"
    provided:
      enabled: "false"
    block:
      deletion:
        increment: "1000"
    send:
      qop:
        enabled: "false"
    blockreport:
      queue:
        size: "1024"
    storage:
      dir:
        perm: "700"

  datanode:
    hostname: "NULL"
    port: "9866"
    address: "?${DFS_DATANODE_HOSTNAME},${DFS_DATANODE_HOSTNAME}:${DFS_DATANODE_PORT},${HOSTNAME}:${DFS_DATANODE_PORT}"
    http:
      port: "9864"
      address: "?${DFS_DATANODE_HOSTNAME},${DFS_DATANODE_HOSTNAME}:${DFS_DATANODE_HTTP_PORT},${HOSTNAME}:${DFS_DATANODE_HTTP_PORT}"
      internal-proxy:
        port: "0"
    https:
      port: "9865"
      address: "?${DFS_DATANODE_HOSTNAME},${DFS_DATANODE_HOSTNAME}:${DFS_DATANODE_HTTPS_PORT},${HOSTNAME}:${DFS_DATANODE_HTTPS_PORT}"
    ipc:
      port: "9867"
      address: "?${DFS_DATANODE_HOSTNAME},${DFS_DATANODE_HOSTNAME}:${DFS_DATANODE_IPC_PORT},${HOSTNAME}:${DFS_DATANODE_IPC_PORT}"
    handler:
      count: "10"
    dns:
      interface: "default"
      namesercer: "default"
    du:
      reserved:
        "": "0"
        calculator: "org.apache.hadoop.hdfs.server.datanode.fsdataset.impl.ReservedSpaceCalculator\\$ReservedSpaceCalculatorAbsolute"
        pct: "0"
    data:
      dir:
        "": "=file://${HADOOP_TMP_DIR}/dfs/data"
        perm: "700"
    directoryscan:
      interval: "21600s"
      threads: "1"
      throttle:
        limit:
          ms:
            per:
              sec: "1000"
    lifeline:
      interval:
        seconds: "NULL"
    balance:
      bandwidthPerSec: "10m"
      max:
        concurrent:
          moves: "50"
    plugins: "NULL"
    failed:
      volumes:
        tolerated: "0"
    volumes:
      replica-add:
        threadpool:
          size: "NULL"
    max:
      transfer:
        threads: "4096"
      locked:
        memory: "0"
    scan:
      period:
        hours: "504"
    readahead:
      bytes: "4194304"
    drop:
      cache:
        behind:
          reads: "false"
          writes: "false"
    sync:
      behind:
        writes:
          "": "false"
          in:
            background: "false"
    use:
      datanode:
        hostname: "false"
    shared:
      file:
        descriptor:
          paths: "/dev/shm,/tmp"
    kerberos:
      principal: "NULL"
    keytab:
      file: "NULL"
    metrics:
      logger:
        period:
          seconds: "600"
    peer:
      stats:
        enabled: "false"
      metrics:
        min:
          outlier:
            detection:
              samples: "1000"
    outliers:
      report:
        interval: "30m"
    fileio:
      profiling:
        sampling:
          percentage: "0"
    available-space-volume-choosing-policy:
      balanced-space-threshold: "10737418240"
      balanced-space-preference-fraction: "0.75f"
    fsdatasetcache:
      max:
        threads:
          per:
            volume: "4"
    slow:
      io:
        warning:
          threshold:
            ms: "300"
    block:
      id:
        layout:
          upgrade:
            threads: "12"
    cache:
      revocation:
        timeout:
          ms: "900000"
        polling:
          ms: "500"
    block-pinning:
      enabled: "false"
    ec:
      reconstruction:
        stripedread:
          timeout:
            millis: "5000"
          buffer:
            size: "65536"
        threads: "8"
        xmits:
          weight: "0.5"
    transfer:
      socket:
        send:
          buffer:
            size: "0"
        recv:
          buffer:
            size: "0"
    bp-ready:
      timeout: "20s"
    cached-dfsused:
      check:
        interval:
          ms: "600000"
    fsdataset:
      factory: "NULL"
      volume:
        choosing:
          policy: "NULL"
    lazywriter:
      interval:
        sec: "60"
    network:
      counts:
        cache:
          max:
            size: "2147483647"
    oob:
      timeout-ms: "1500,0,0,0"
    parallel:
      volumes:
        load:
          threads:
            num: "NULL"
    ram:
      disk:
        replica:
          tracker: "NULL"
    restart:
      replica:
        expiration: "50"
    socket:
      reuse:
        keepalive: "4000"
      write:
        timeout: "480000"
    transferTo:
      allowed: "true"
    disk:
      check:
        min:
          gap: "15m"
        timeout: "10m"

  http:
    policy: "HTTP_ONLY"
    client:
      retry:
        policy:
          enabled: "false"
          spec: "10000,6,60000,10"
        max:
          attempts: "10"
      failover:
        max:
          attempts: "15"
        sleep:
          base:
            millis: "500"
          max:
            millis: "15000"

  https:
    server:
      keystore:
        resource: "ssl-server.xml"

  client:
    https:
      need-auth: "false"
      keystore:
        resource: "ssl-client.xml"
    cached:
      conn:
        retry: "3"
    block:
      write:
        replace-datanode-on-failure:
          enable: "true"
          policy: "DEFAULT"
          best-effort: "false"
          min-replication: "0"
        locateFollowingBlock:
          initial:
            delay:
              ms: "400"
          retries: "5"
    write:
      exclude:
        nodes:
          cache:
            expiry:
              interval:
                millis: "600000"
      byte-array-manager:
        count-limit: "2048"
        count-reset-time-period-ms: "10000"
        count-threshold: "128"
        enabled: "false"
      max-packets-in-flight: "80"
    failover:
      max:
        attempts: "15"
      sleep:
        base:
          millis: "500"
        max:
          millis: "15000"
      connection:
        retries:
          "": "0"
          "on":
            timeouts: "0"
      proxy:
        provider: "NULL"
      random:
        order: "false"
    datanode-restart:
      timeout: "30"
    use:
      datanode:
        hostname: "false"
      legacy:
        blockreader:
          local: "false"
    local:
      interfaces: "NULL"
    cache:
      drop:
        behind:
          writes: "NULL"
          reads: "NULL"
      readahead: "NULL"
    server-defaults:
      validity:
        period:
          ms: "3600000"
    mmap:
      enabled: "true"
      cache:
        size: "256"
        timeout:
          ms: "3600000"
      retry:
        timeout:
          ms: "300000"
    short:
      circuit:
        replica:
          stale:
            threshold:
              ms: "1800000"
    context: "default"
    read:
      shortcircuit:
        "": "false"
        skip:
          checksum: "false"
        streams:
          cache:
            size: "256"
            expiry:
              ms: "300000"
        buffer:
          size: "1048576"
      prefetch:
        size: "NULL"
      short:
        circuit:
          replica:
            stale:
              threshold:
                ms: "1800000"
      striped:
        threadpool:
          size: "18"
    socket:
      send:
        buffer:
          size: "0"
    domain:
      socket:
        data:
          traffic: "false"
    slow:
      io:
        warning:
          threshold:
            ms: "30000"
    key:
      provider:
        cache:
          expiry: "864000000"
    max:
      block:
        acquire:
          failures: "3"
    replica:
      accessor:
        builder:
          classes: "NULL"
    retry:
      interval-ms:
        get-last-block-length: "4000"
      max:
        attempts: "10"
      policy:
        enabled: "false"
        spec: "10000,6,60000,10"
      times:
        get-last-block-length: "3"
      window:
        base: "3000"
    socket-timeout: "60000"
    socketcache:
      capacity: "16"
      expiryMsec: "3000"
    test:
      drop:
        namenode:
          response:
            number: "0"
    hedged:
      read:
        threadpool:
          size: "0"
        threshold:
          millis: "500"

  default:
    chunk:
      view:
        size: "32768"

  permissions:
    enabled: "true"
    ContentSummary:
      subAccess: "false"
    superusergroup: "supergroup"

  cluster:
    administrators: "NULL"

  block:
    access:
      token:
        enable: "false"
        lifetime: "600"
        protobuf:
          enable: "false"
      key:
        update:
          interval: "600"
    scanner:
      volume:
        bytes:
          per:
            second: "1048576"
    local-path-access:
      user: "NULL"
    invalidate:
      limit: "1000"
    misreplication:
      processing:
        limit: "10000"
    placement:
      ec:
        classname: "org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyRackFaultTolerant"
    replicator:
      classname: "org.apache.hadoop.hdfs.server.blockmanagement.BlockPlacementPolicyDefault"

  replication:
    "": "3"
    max: "512"

  blockreport:
    intervalMsec: "21600000"
    initialDelay: "0s"
    split:
      threshold: "1000000"
    incremental:
      intervalMsec: "0"

  heartbeat:
    interval: "3s"

  hosts:
    "": "NULL"
    exclude: "NULL"

  image:
    compress: "false"
    compression:
      codec: "org.apache.hadoop.io.compress.DefaultCodec"
    transfer:
      timeout: "60000"
      bandwidthPerSec: "0"
      chunksize: "65536"
    transfer-bootstrap-standby:
      bandwidthPerSec: "0"

  edit:
    log:
      transfer:
        timeout: "30000"
        bandwidthPerSec: "0"

  nameservices:
    "": "NULL"

  nameservice:
    id: "NULL"

  internal:
    nameservices: "NULL"

  ha:
    namenodes:
      EXAMPLENAMESERVICE: "NULL"
    namenode:
      id: "NULL"
    log-roll:
      period: "120s"
    tail-edits:
      period:
        "": "60s"
        backoff-max: "0"
      namenode-retries: "3"
      rolledits:
        timeout: "60"
      in-progress: "false"
    automatic-failover:
      enabled: "false"
    zkfc:
      nn:
        http:
          timeout:
            ms: "20000"
      port: "8019"
    fencing:
      methods: "NULL"
    standby:
      checkpoints: "true"

  short:
    circuit:
      shared:
        memory:
          watcher:
            interrupt:
              check:
                ms: "60000"

  journalnode:
    hostname: "NULL"
    kerberos:
      principal: "NULL"
      internal:
        spnego:
          principal: "=${DFS_WEB_AUTHENTICATION_KERBEROS_PRINCIPAL}"
    keytab:
      file: "NULL"
    rpc-port: "8485"
    rpc-address: "?${DFS_JOURNALNODE_HOSTNAME},${DFS_JOURNALNODE_HOSTNAME}:${DFS_JOURNALNODE_RPC_PORT},${HOSTNAME}:${DFS_JOURNALNODE_RPC_PORT}"
    rpc-bind-host: "0.0.0.0"
    http-port: "8480"
    http-address: "?${DFS_JOURNALNODE_HOSTNAME},${DFS_JOURNALNODE_HOSTNAME}:${DFS_JOURNALNODE_HTTP_PORT},${HOSTNAME}:${DFS_JOURNALNODE_HTTP_PORT}"
    http-bind-host: "0.0.0.0"
    https-port: "8481"
    https-address: "?${DFS_JOURNALNODE_HOSTNAME},${DFS_JOURNALNODE_HOSTNAME}:${DFS_JOURNALNODE_HTTPS_PORT},${HOSTNAME}:${DFS_JOURNALNODE_HTTPS_PORT}"
    https-bind-host: "0.0.0.0"
    edits:
      dir:
        "": "/tmp/hadoop/dfs/journalnode/"
        perm: "700"
    enable:
      sync: "true"
    sync:
      interval: "120000"
    edit-cache-size:
      bytes: "1048576"

  secondary:
    namenode:
      kerberos:
        internal:
          spnego:
            principal: "=${DFS_WEB_AUTHENTICATION_KERBEROS_PRINCIPAL}"
        principal: "NULL"
      keytab:
        file: "NULL"

  web:
    authentication:
      kerberos:
        principal: "NULL"
        keytab: "NULL"
      filter: "org.apache.hadoop.hdfs.web.AuthFilter"
      simple:
        anonymous:
          allowed: "NULL"
    ugi: "NULL"

  metrics:
    percentiles:
      intervals: "NULL"

  encrypt:
    data:
      transfer:
        "": "false"
        algorithm: "NULL"
        cipher:
          suites: "NULL"
          key:
            bitlength: "128"
      overwrite:
        downstream:
          derived:
            qop: "false"
          new:
            qop: "NULL"

  trustedchannel:
    resolver:
      class: "NULL"

  data:
    transfer:
      protection: "NULL"
      saslproperties:
        resolver:
          class: "NULL"
      client:
        tcpnodelay: "true"
      server:
        tcpnodelay: "true"

  cachereport:
    intervalMsec: "10000"

  webhdfs:
    user:
      provider:
        user:
          pattern: "^[A-Za-z_][A-Za-z0-9._-]*[$]?$"
    acl:
      provider:
        permission:
          pattern: "^(default:)?(user|group|mask|other):[[A-Za-z_][A-Za-z0-9._-]]*:([rwx-]{3})?(,(default:)?(user|group|mask|other):[[A-Za-z_][A-Za-z0-9._-]]*:([rwx-]{3})?)*$"
    socket:
      connect-timeout: "60s"
      read-timeout: "60s"
    ugi:
      expire:
        after:
          access: "600000"
    rest-csrf:
      enabled: "false"
      custom-header: "X-XSRF-HEADER"
      methods-to-ignore: "GET,OPTIONS,HEAD,TRACE"
      browser-useragents-regex: "^Mozilla.*,^Opera.*"
    netty:
      high:
        watermark: "65535"
      low:
        watermark: "32768"
    oauth2:
      access:
        token:
          provider: "NULL"
      client:
        id: "NULL"
      enabled: "false"
      refresh:
        url: "NULL"
    use:
      ipc:
        callq: "true"

  domain:
    socket:
      path: "NULL"
      disable:
        interval:
          seconds: "600"

  user:
    home:
      dir:
        prefix: "/user"

  storage:
    policy:
      enabled: "true"
      satisfier:
        enabled: "false"
        queue:
          limit: "1000"
        work:
          multiplier:
            per:
              iteration: "1"
        recheck:
          timeout:
            millis: "60000"
        self:
          retry:
            timeout:
              millis: "300000"
        retry:
          max:
            attempts: "3"
        datanode:
          cache:
            refresh:
              interval:
                ms: "300000"
        max:
          outstanding:
            paths: "10000"
        hostname: "NULL"
        port: "0"
        address: "?{DFS_STORAGE_POLICY_SATISFIER_HOSTNAME},${DFS_STORAGE_POLICY_SATISFIER_HOSTNAME}:${DFS_STORAGE_POLICY_SATISFIER_PORT},${HOSTNAME}:${DFS_STORAGE_POLICY_SATISFIER_PORT}"
        keytab:
          file: "NULL"
        kerberos:
          principal: "NULL"

  xframe:
    enabled: "true"
    value: "SAMEORIGIN"

  balancer:
    keytab:
      enabled: "false"
      file: "NULL"
    hostname: "NULL"
    port: "0"
    address: "?${DFS_BALANCER_HOSTNAME},${DFS_BALANCER_HOSTNAME}:${DFS_BALANCER_PORT},${HOSTNAME}:${DFS_BALANCER_PORT}"
    kerberos:
      principal: "NULL"
    dispatcherThreads: "200"
    movedWinWidth: "5400000"
    moverThreads: "1000"
    max-size-to-move: "10737418240"
    getBlocks:
      min-block-size: "10485760"
      size: "2147483648"
    block-move:
      timeout: "0"
    max-no-move-interval: "60000"
    max-iteration-time: "1200000"

  checksum:
    type: "CRC32C"
    combine:
      mode: "MD5MD5CRC"

  content-summary:
    limit: "5000"
    sleep-microsec: "500"

  ls:
    limit: "1000"

  mover:
    movedWinWidth: "5400000"
    moverThreads: "1000"
    retry:
      max:
        attempts: "10"
    keytab:
      enabled: "false"
      file: "NULL"
    hostname: "NULL"
    port: "0"
    address: "?${DFS_MOVER_HOSTNAME},${DFS_MOVER_HOSTNAME}:${DFS_MOVER_PORT},${HOSTNAME}:${DFS_MOVER_PORT}"
    kerberos:
      principal: "NULL"
    max-no-move-interval: "60000"

  pipeline:
    ecn: "false"

  qjournal:
    accept-recovery:
      timeout:
        ms: "120000"
    finalize-segment:
      timeout:
        ms: "120000"
    get-journal-state:
      timeout:
        ms: "120000"
    new-epoch:
      timeout:
        ms: "120000"
    prepare-recovery:
      timeout:
        ms: "120000"
    queued-edits:
      limit:
        mb: "10"
    select-input-streams:
      timeout:
        ms: "20000"
    start-segment:
      timeout:
        ms: "20000"
    write-txns:
      timeout:
        ms: "20000"
    http:
      open:
        timeout:
          ms: "60000"
      read:
        timeout:
          ms: "60000"

  quota:
    by:
      storage:
        type:
          enabled: "true"

  disk:
    balancer:
      max:
        disk:
          throughputInMBperSec: "10"
          errors: "5"
      block:
        tolerance:
          percent: "10"
      plan:
        valid:
          interval: "1d"
        threshold:
          percent: "10"
      enabled: "true"

  provided:
    storage:
      id: "DS-PROVIDED"
    aliasmap:
      class: "org.apache.hadoop.hdfs.server.common.blockaliasmap.impl.TextFileRegionAliasMap"
      inmemory:
        batch-size: "500"
        dnrpc-address: "NULL"
        rpc-bind-host: "0.0.0.0"
        leveldb:
          dir: "/tmp"
        enabled: "false"
        server:
          log: "false"
      text:
        delimiter: "NULL"
        read:
          file: "NULL"
        codec: "NULL"
        write:
          dir: "NULL"
      leveldb:
        path: "NULL"
      load:
        retries: "0"

  lock:
    suppress:
      warning:
        interval: "10s"

  use:
    dfs:
      network:
        topology: "true"

  net:
    topology:
      impl: "org.apache.hadoop.hdfs.net.DFSNetworkTopology"

  qjm:
    operations:
      timeout: "60s"

  reformat:
    disabled: "false"

  blocksize: "134217728"

  stream-buffer-size: "4096"

  bytes-per-checksum: "512"

  client-write-packet-size: "65536"

nfs:
  server:
    port: "2049"
  mountd:
    port: "4242"
  dump:
    dir: "/tmp/.hdfs-nfs"
  rtmax: "1048576"
  wtmax: "1048576"
  keytab:
    file: "NULL"
  kerberos:
    principal: "NULL"
  allow:
    insecure:
      ports: "true"

hadoop:
  fuse:
    connection:
      timeout: "300"
    timer:
      period: "5"
  user:
    group:
      metrics:
        percentiles:
          intervals: "NULL"

datanode:
  https:
    port: "50475"

ssl:
  server:
    keystore:
      keypassword: "NULL"
      location: "NULL"
      password: "NULL"
    truststore:
      location: "NULL"
      password: "NULL"

https:
  buffer:
    size: "4096"