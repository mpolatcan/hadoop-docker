hadoop:
  tmp:
    dir: "=${HDUSER_HOME}/app/hadoop/tmp"

  http:
    filter:
      initializers: "org.apache.hadoop.http.lib.StaticUserWebFilter"
    authentication:
      type: "simple"
      token:
        validity: "36000"
      signature:
        secret:
          file: "=${HDUSER_HOME}/hadoop-http-auth-signature-secret"
      cookie:
        domain: "NULL"
      simple:
        anonymous:
          allowed: "true"
      kerberos:
        principal: "HTTP/_HOST@LOCALHOST"
        keytab: "=${HDUSER_HOME}/hadoop.keytab"
    cross-origin:
      enabled: "false"
      allowed-origins: "*"
      allowed-methods: "GET,POST,HEAD"
      allowed-headers: "X-Requested-With,Content-Type,Accept,Origin"
      max-age: "1800"
    staticuser:
      user: "dr.who"
    logs:
      enabled: "true"

  security:
    authorization: "false"
    instrumentation:
      requires:
        admin: "false"
    authentication: "simple"
    group:
      mapping:
        "": "org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback"
        ldap:
          connection:
            timeout:
              ms: "60000"
          read:
            timeout:
              ms: "60000"
          num:
            attempts:
              "": "3"
              before:
                failover: "3"
          url: "NULL"
          ssl:
            "": "false"
            keystore:
              "": "NULL"
              password:
                "": "NULL"
                file: "NULL"
            truststore:
              "": "NULL"
              password:
                file: "NULL"
          conversion:
            rule: "none"
          bind:
            user: "NULL"
            password:
              "": "NULL"
              file: "NULL"
          base: "NULL"
          userbase: "NULL"
          groupbase: "NULL"
          search:
            filter:
              user: "NULL"
              group: "NULL"
            attr:
              member: "member"
              memberof: "NULL"
              group:
                name: "cn"
            group:
              hierarchy:
                levels: "0"
          posix:
            attr:
              uid:
                name: "uidNumber"
              gid:
                name: "gidNumber"
          directory:
            search:
              timeout: "10000"
        providers:
          combined: "true"
      service:
        user:
          name:
            key: "NULL"
      uid:
        cache:
          secs: "14400"
    credential:
      clear-text-fallback: "true"
      provider:
        path: "NULL"
    credstore:
      java-keystore-provider:
        password-file: "NULL"
    dns:
      interface: "NULL"
      nameserver: "NULL"
      log-slow-lookups:
        enabled: "false"
        threshold:
          ms: "1000"
    groups:
      cache:
        secs: "300"
        warn:
          after:
            ms: "5000"
        background:
          reload:
            "": "false"
            threads: "3"
        shell:
          command:
            timeout: "0s"
      negative-cache:
        secs: "30"
    uid:
      cache:
        secs: "14400"
    saslproperties:
      resolver:
        class: "NULL"
    sensitive-config-keys: "secret$ password$ ssl.keystore.pass$ fs.s3a.server-side-encryption.key fs.s3a.*.server-side-encryption.key fs.s3a.secret.key fs.s3a.*.secret.key fs.s3a.session.key fs.s3a.*.session.key fs.s3a.session.token fs.s3a.*.session.token fs.azure.account.key.* fs.azure.oauth2.* fs.adl.oauth2.* credential$ oauth.*secret oauth.*password oauth.*token hadoop.security.sensitive-config-keys"
    auth_to_local:
      "": "NULL"
      mechanism: "hadoop"
    impersonation:
      provider:
        class: "NULL"
    crpyto:
      codec:
        classes:
          EXAMPLECIPHERSUITE: "NULL"
          aes:
            ctr:
              nopadding: "org.apache.hadoop.crypto.OpensslAesCtrCryptoCodec, org.apache.hadoop.crypto.JceAesCtrCryptoCodec"
      cipher:
        suite: "AES/CTR/NoPadding"
      jce:
        provider: "NULL"
      jceks:
        key:
          serialfilter: "NULL"
      buffer:
        size: "8192"
    java:
      secure:
        random:
          algorithm: "SHA1PRNG"
    secure:
      random:
        impl: "NULL"
    random:
      device:
        file:
          path: "/dev/urandom"
    key:
      provider:
        path: "NULL"
      default:
        bitlength: "128"
        cipher: "AES/CTR/NoPadding"
    kms:
      client:
        authentication:
          retry-count: "1"
        encrypted:
          key:
            cache:
              size: "500"
              low-watermark: "0.3f"
              num:
                refill:
                  threads: "2"
              expiry: "43200000"
        timeout: "60"
        failover:
          sleep:
            base:
              millis: "100"
            max:
              millis: "2000"

  service:
    shutdown:
      timeout: "30s"

  rpc:
    protection: "authentication"
    socket:
      factory:
        class:
          default: "org.apache.hadoop.net.StandardSocketFactory"
          ClientProtocol: "NULL"

  socks:
    server: "NULL"

  workaround:
    non:
      threadsafe:
        getpwuid: "true"

  kerberos:
    kinit:
      command: "kinit"
    min:
      seconds:
        before:
          relogin: "60"

  token:
    files: "NULL"

  util:
    hash:
      type: "murmur"

  ssl:
    keystores:
      factory:
        class: "org.apache.hadoop.security.ssl.FileBasedKeyStoresFactory"
    require:
      client:
        cert: "false"
    hostname:
      verifier: "DEFAULT"
    server:
      conf: "ssl-server.xml"
    client:
      conf: "ssl-client.xml"
    enabled:
      "": "false"
      protocols: "TLSv1,SSLv2Hello,TLSv1.1,TLSv1.2"

  user:
    group:
      static:
        mapping:
          overrides: "dr.who=;"

  registry:
    zk:
      root: "/registry"
      session:
        timeout:
          ms: "60000"
      connection:
        timeout:
          ms: "15000"
      retry:
        times: "5"
        interval:
          ms: "1000"
        ceiling:
          ms: "60000"
      quorum: "localhost:2181"
    secure: "false"
    system:
      acls: "sasl:yarn@, sasl:mapred@, sasl:hdfs@"
    kerberos:
      realm: "NULL"
    jaas:
      context: "Client"

  shell:
    missing:
      defaultFs:
        warning: "false"
    safely:
      delete:
        limit:
          num:
            files: "100"

  htrace:
    span:
      receiver:
        classes: "NULL"

  caller:
    context:
      enabled: "false"
      max:
        size: "128"
      signature:
        max:
          size: "40"

  zk:
    address: "NULL"
    num-retries: "1000"
    retry-interval-ms: "1000"
    timeout-ms: "10000"
    acl: "world:anyone:rwcda"
    auth: "NULL"

  system:
    tags: "YARN,HDFS,NAMENODE,DATANODE,REQUIRED,SECURITY,KERBEROS,PERFORMANCE,CLIENT,SERVER,DEBUG,DEPRECATED,COMMON,OPTIONAL"

  tags:
    system: "YARN,HDFS,NAMENODE,DATANODE,REQUIRED,SECURITY,KERBEROS,PERFORMANCE,CLIENT,SERVER,DEBUG,DEPRECATED,COMMON,OPTIONAL"

net:
  topology:
    node:
      switch:
        mapping:
          impl: "org.apache.hadoop.net.ScriptBasedMapping"
    impl: "org.apache.hadoop.net.NetworkTopology"
    script:
      file:
        name: "NULL"
      number:
        args: "100"
    table:
      file:
        name: "NULL"

io:
  file:
    buffer:
      size: "4096"
  bytes:
    per:
      checksum: "512"
  skip:
    checksum:
      errors: "false"
  compression:
    codecs:
      "": "NULL"
      bzip2:
        library: "system-native"
  serializations: "org.apache.hadoop.io.serializer.WritableSerialization, org.apache.hadoop.io.serializer.avro.AvroSpecificSerialization, org.apache.hadoop.io.serializer.avro.AvroReflectSerialization"
  seqfile:
    local:
      dir: "=${HADOOP_TMP_DIR}/io/local"
    compress:
      blocksize: "1000000"
  mapfile:
    bloom:
      size: "1048576"
      error:
        rate: "0.005"
  map:
    index:
      skip: "0"
      interval: "128"
  erasurecode:
    codecs:
      rs:
        rawcoders: "rs_native,rs_java"
      rs-legacy:
        rawcoders: "rs-legacy_java"
      xor:
        rawcoders: "xor_native,xor_java"

fs:
  prefix: "hdfs://"
  hostname: "NULL"
  port: "9000"
  defaultFS: "?${FS_HOSTNAME},${FS_PREFIX}${FS_HOSTNAME}:${FS_PORT},${FS_PREFIX}${HOSTNAME}:${FS_PORT}"
  trash:
    interval: "0"
    checkpoint:
      interval: "0"
  protected:
    directories: "NULL"
  AbstractFileSystem:
    file:
      impl: "org.apache.hadoop.fs.local.LocalFs"
    har:
      impl: "org.apache.hadoop.fs.HarFs"
    hdfs:
      impl: "org.apache.hadoop.fs.Hdfs"
    viewfs:
      impl: "org.apache.hadoop.fs.viewfs.ViewFs"
    ftp:
      impl: "org.apache.hadoop.fs.ftp.FtpFs"
    webhdfs:
      impl: "org.apache.hadoop.fs.WebHdfs"
    swebhdfs:
      impl: "org.apache.hadoop.fs.SWebHdfs"
    s3a:
      impl: "org.apache.hadoop.fs.s3a.S3A"
    wasb:
      impl: "org.apache.hadoop.fs.azure.Wasb"
    wasbs:
      impl: "org.apache.hadoop.fs.azure.Wasbs"
    abfs:
      impl: "org.apache.hadoop.fs.azurebfs.Abfs"
    abfss:
      impl: "org.apache.hadoop.fs.azurebfs.Abfss"
    adl:
      impl: "org.apache.hadoop.fs.adl.Adl"
  viewfs:
    rename:
      strategy: "SAME_MOUNTPOINT"
  ftp:
    impl: "org.apache.hadoop.fs.ftp.FTPFileSystem"
    host:
      "": "0.0.0.0"
      port: "21"
    data:
      connection:
        mode: "ACTIVE_LOCAL_DATA_CONNECTION_MODE"
    transfer:
      mode: "BLOCK_TRANSFER_MODE"
  df:
    interval: "60000"
  du:
    interval: "600000"
  swift:
    impl: "org.apache.hadoop.fs.swift.snative.SwiftNativeFileSystem"
  automatic:
    close: "true"
  s3a:
    access:
      key: "NULL"
    secret:
      key: "NULL"
    aws:
      credentials:
        provider: "NULL"
    session:
      token: "NULL"
    security:
      credential:
        provider:
          path: "NULL"
    assumed:
      role:
        arn: "NULL"
        session:
          name: "NULL"
          duration: "30m"
        policy: "NULL"
        sts:
          endpoint:
            "": "NULL"
            region: "us-west-1"
        credentials:
          provider: "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider"
    connection:
      maximum: "15"
      ssl:
        enabled: "true"
      establish:
        timeout: "5000"
      timeout: "200000"
    endpoint: "NULL"
    path:
      style:
        access: "false"
    proxy:
      host: "NULL"
      port: "NULL"
      username: "NULL"
      password: "NULL"
      workstation: "NULL"
    attempts:
      maximum: "20"
    socket:
      send:
        buffer: "8192"
      recv:
        buffer: "8192"
    paging:
      maximum: "5000"
    threads:
      max: "10"
      keepalivetime: "60"
    max:
      total:
        tasks: "5"
    multipart:
      size: "100M"
      threshold: "2147483647"
      purge:
        "": "false"
        age: "86400"
    multiobjectdelete:
      enable: "true"
    acl:
      default: "NULL"
    server-side-encryption-algorithm: "NULL"
    server-side-encryption:
      key: "NULL"
    signing-algorithm: "NULL"
    block:
      size: "NULL"
    buffer:
      dir: "=${HADOOP_TMP_DIR}/s3a"
    fast:
      upload:
        buffer: "disk"
        active:
          blocks: "4"
    readahead:
      range: "64K"
    user:
      agent:
        prefix: "NULL"
    metadatastore:
      authoritative: "false"
      impl: "org.apache.hadoop.fs.s3a.s3guard.NullMetadataStore"
    s3aguard:
      ddb:
        region: "NULL"
        table:
          "": "NULL"
          create: "false"
          capacity:
            read: "500"
            write: "100"
        max:
          retries: "9"
        throttle:
          retry:
            interval: "100ms"
        background:
          sleep: "25ms"
    retry:
      limit: "=${FS_S3A_ATTEMPTS_MAXIMUM}"
      interval: "500ms"
      throttle:
        limit: "=${FS_S3A_ATTEMPTS_MAXIMUM}"
        interval: "1000ms"
    committer:
      name: "file"
      magic:
        enabled: "false"
      threads: "8"
      staging:
        tmp:
          path: "tmp/staging"
        unique-filenames: "true"
        conflict-mode: "fail"
        abort:
          pending:
            uploads: "true"
    list:
      version: "2"
    etag:
      checksum:
        enabled: "false"
    change:
      detection:
        source: "etag"
        mode: "server"
        version:
          required: "true"
  wasb:
    impl: "org.apache.hadoop.fs.azure.NativeAzureFileSystem"
  wasbs:
    impl: "org.apache.hadoop.fs.azure.NativeAzureFileSystem$Secure"
  azure:
    secure:
      mode: "false"
    local:
      sas:
        key:
          mode: "false"
        expiry:
          period: "90d"
    authorization:
      "": "false"
      caching:
        enable: "true"
    saskey:
      usecontainersaskeyforallaccess: "true"
  abfs:
    impl: "org.apache.hadoop.fs.azurebfs.AzureBlobFileSystem"
  abfss:
    impl: "org.apache.hadoop.fs.azurebfs.SecureAzureBlobFileSystem"
  permissions:
    umask-mode: "022"
  client:
    resolve:
      remote:
        symlinks: "true"
      topology:
        enabled: "false"
    htrace:
      sampler:
        classes: "NULL"
  har:
    impl:
      disable:
        cache: "true"
  adl:
    impl: "org.apache.hadoop.fs.adl.AdlFileSystem"
    oauth2:
      access:
        token:
          provider:
            "": "NULL"
            type: "ClientCredential"
      client:
        id: "NULL"
      credential: "NULL"
      refresh:
        url: "NULL"
        token: "NULL"
      msi:
        port: "NULL"
      devicecode:
        clientapp:
          id: "NULL"

ipc:
  client:
    idlethreshold: "4000"
    kill:
      max: "10"
    connection:
      maxidletime: "10000"
    connect:
      max:
        retries:
          "": "10"
          "on":
            timeouts: "45"
      retry:
        interval: "1000"
      timeout: "20000"
    tcpnodelay: "true"
    low-latency: "false"
    ping: "true"
    rpc-timeout:
      ms: "0"
    fallback-to-simple-auth-allowed: "false"
    bind:
      wildcard:
        addr: "false"
  ping:
    interval: "60000"
  server:
    listen:
      queue:
        size: "128"
    log:
      slow:
        rpc: "false"
    max:
      connections: "0"
  maximum:
    data:
      length: "67108864"
    response:
      length: "134217728"

file:
  stream-buffer-size: "4096"
  bytes-per-checksum: "512"
  client-write-packet-size: "65536"
  blocksize: "67108864"
  replication: "1"

ftp:
  stream-buffer-size: "4096"
  bytes-per-checksum: "512"
  client-write-packet-size: "65536"
  blocksize: "67108864"
  replication: "3"

tfile:
  io:
    chunk:
      size: "1048576"
  fs:
    output:
      buffer:
        size: "262144"
    input:
      buffer:
        size: "262144"

dfs:
  ha:
    fencing:
      methods: "NULL"
      ssh:
        connect-timeout: "30000"
        private-key-files: "NULL"

ha:
  zookeeper:
    quorum: "NULL"
    session-timeout:
      ms: "10000"
    parent-znode: "/hadoop-ha"
    acl: "world:anyone:rwcda"
    auth: "NULL"
  health-monitor:
    connect-retry-interval:
      ms: "1000"
    check-interval:
      ms: "1000"
    sleep-after-disconnect:
      ms: "1000"
    rpc-timeout:
      ms: "45000"
  failover-controller:
    new-active:
      rpc-timeout:
        ms: "60000"
    graceful-fence:
      rpc-timeout:
        ms: "5000"
      connection:
        retries: "1"
    cli-check:
      rpc-timeout:
        ms: "20000"

jetty:
  logs:
    serve:
      aliases: "true"

nfs:
  exports:
    allowed:
      hosts: "*rw"

rpc:
  metrics:
    quantile:
      enable: "false"
    percentiles:
      intervals: "NULL"

adl:
  feature:
    ownerandgroup:
      enableupn: "false"
  http:
    timeout: "-1"

seq:
  io:
    sort:
      mb: "100"
      factor: "100"