mapreduce:
  job:
    hdfs-servers: "?${FS_HOSTNAME},${FS_PREFIX}${FS_HOSTNAME}:${FS_PORT},${FS_PREFIX}${HOSTNAME}:${FS_PORT}"
    committer:
      setup:
        cleanup:
          needed: "true"
    local-fs:
      single-disk-limit:
        bytes: "-1"
        check:
          interval-ms: "5000"
          kill-limit-exceed: "true"
    maps: "2"
    reduces: "1"
    running:
      map:
        limit: "0"
      reduce:
        limit: "1"
    max:
      map: "-1"
      split:
        locations: "15"
    reducer:
      preempt:
        delay:
          sec: "0"
      unconditional-preempt:
        delay:
          sec: "300"
    split:
      metainfo:
        maxsize: "10000000"
    speculative:
      speculative-cap-runninng-tasks: "0.1"
      speculative-cap-total-tasks: "0.01"
      minimum-allowed-tasks: "10"
      retry-after-no-speculate: "1000"
      retry-after-speculate: "15000"
      slowtaskthreshold: "1.0"
    map:
      output:
        collector:
          class: "org.apache.hadoop.mapred.MapTask\\$MapOutputBuffer"
    ubertask:
      enable: "false"
      maxmaps: "9"
      maxreduces: "1"
      maxbytes: "NULL"
    emit-timeline-data: "false"
    sharedcache:
      mode: "disabled"
    maxtaskfailures:
      per:
        tracker: "3"
    skip:
      outdir: "NULL"
    queuename: "default"
    tags: "NULL"
    acl-modify-job: "NULL"
    acl-view-job: "NULL"
    finish-when-all-reducers-done: "true"
    token:
      tracking:
        ids:
          "": "NULL"
          enabled: "false"
    reduce:
      slowstart:
        completedmaps: "0.05"
      shuffle:
        consumer:
          plugin:
            class: "org.apache.hadoop.mapreduce.task.reduce.Shuffle"
    complete:
      cancel:
        delegation:
          tokens: "true"
    node-label-expression: "NULL"
    am:
      node-label-expression: "NULL"
    counters:
      limit: "120"
    end-notification:
      url: "NULL"
      retry:
        attempts: "0"
        interval: "1000"
      max:
        attempts: "5"
        retry:
          interval: "5000"
    log4j-properties-file: "NULL"
    classloader:
      "": "false"
      system:
        classes: "NULL"
    heap:
      memory-mb:
        ratio: "0.8"
    encrypted-intermediate-data:
      "": "false"
      buffer:
        kb: "128"
    encrypted-intermediate-data-key-size-bits: "128"
    cache:
      limit:
        max-resources: "0"
        max-resources-mb: "0"
        max-single-resource-mb: "0"
    redacted-properties: "NULL"
    send-token-conf: "NULL"

  input:
    fileinputformat:
      split:
        minsize: "0"
      list-status:
        num-threads: "1"
    lineinputformat:
      linespermap: "1"

  client:
    submit:
      file:
        replication: "10"
    output:
      filter: "FAILED"
    completion:
      pollinterval: "5000"
    progressmonitor:
      pollinterval: "1000"
    libjars:
      wildcard: "true"

  task:
    io:
      sort:
        factor: "10"
        mb: "100"
    timeout: "600000"
    files:
      preserve:
        failedtasks: "false"
    userlog:
      limit:
        kb: "0"
    profile:
      "": "false"
      maps: "0-2"
      reduces: "0-2"
      params: "-agentlib:hprof=cpu=samples,heap=sites,force=n,thread=y,verbose=n,file=%s"
      map:
        params: "=${MAPREDUCE_TASK_PROFILE_PARAMS}"
      reduce:
        params: "=${MAPREDUCE_TASK_PROFILE_PARAMS}"
      skip:
        start:
          attempts: "2"
    merge:
      progress:
        records: "10000"
    combine:
      progress:
        records: "10000"
    exit:
      timeout:
        "": "60000"
        check-interval-ms: "20000"
    local-fs:
      write-limit:
        bytes: "-1"

  output:
    fileoutputformat:
      compress:
        "": "false"
        type: "RECORD"
        codec: "org.apache.hadoop.io.compress.DefaultCodec"

  map:
    sort:
      spill:
        percent: "0.80"
    maxattempts: "4"
    memory:
      mb: "-1"
    cpu:
      vcores: "1"
    log:
      level: "INFO"
    speculative: "true"
    skip:
      maxrecords: "0"
      proc-count:
        auto-incr: "true"
    node-label-expression: "NULL"
    env: "=HADOOP_MAPRED_HOME=${HADOOP_HOME}"

  reduce:
    maxattempts: "4"
    shuffle:
      fetch:
        retry:
          enabled: "=${YARN_NODEMANAGER_RECOVERY_ENABLED}"
          interval-ms: "1000"
          timeout-ms: "30000"
      retry-delay:
        max:
          ms: "60000"
      parallelcopies: "5"
      connect:
        timeout: "180000"
      read:
        timeout: "180000"
      merge:
        percent: "0.66"
      input:
        buffer:
          percent: "0.70"
      memory:
        limit:
          percent: "0.25"
      ssl:
        enabled: "false"
        file:
          buffer:
            size: "65536"
      max:
        connections: "0"
        threads: "0"
      transferTo:
        allowed: "NULL"
      transfer:
        buffer:
          size: "131072"
    markreset:
      buffer:
        percent: "0.0"
    input:
      buffer:
        percent: "0.0"
    memory:
      mb: "-1"
    cpu:
      vcores: "1"
    log:
      level: "INFO"
    merge:
      inmem:
        threshold: "1000"
    speculative: "true"
    skip:
      maxgroups: "0"
      proc-count:
        auto-incr: "true"
    node-label-expression: "NULL"
    env: "=HADOOP_MAPRED_HOME=${HADOOP_HOME}"

  shuffle:
    listen:
      queue:
        size: "128"
    connection-keep-alive:
      enable: "false"
      timeout: "5"
    port: "13562"

  ifile:
    readahead:
      "": "true"
      bytes: "4194304"

  cluster:
    local:
      dir: "=${HADOOP_TMP_DIR}/mapred/local"
    acls:
      enabled: "false"

  framework:
    name: "yarn"

  am:
    max-attempts: "2"

  fileoutputcommitter:
    algorithm:
      version: "2"
    task:
      cleanup:
        enabled: "false"

  application:
    classpath: "NULL"
    framework:
      path: "NULL"

  app-submission:
    cross-platform: "false"

  jvm:
    system-properties-to-log: "os.name,os.version,java.home,java.runtime.version,java.vendor,java.version,java.vm.name,java.class.path,java.io.tmpdir,user.dir,user.name"

  jobhistory:
    hostname: "NULL"
    port: "10020"
    address: "?${MAPREDUCE_JOBHISTORY_HOSTNAME},${MAPREDUCE_JOBHISTORY_HOSTNAME}:${MAPREDUCE_JOBHISTORY_PORT},${HOSTNAME}:${MAPREDUCE_JOBHISTORY_PORT}"
    webapp:
      port: "19888"
      address: "=${WEBAPP_HOSTNAME}:${MAPREDUCE_JOBHISTORY_WEBAPP_PORT}"
      https:
        port: "19890"
        address: "=${WEBAPP_HOSTNAME}:${MAPREDUCE_JOBHISTORY_WEBAPP_HTTPS_PORT}"
      rest-csrf:
        enabled: "false"
        custom-header: "X-XSRF-Header"
        methods-to-ignore: "GET,OPTIONS,HEAD"
      xfs-filter:
        xframe-options: "SAMEORIGIN"
    keytab: "/etc/security/keytab/jhs.service.keytab"
    principal: "jhs/_HOST@REALM.TLD"
    intermediate-done-dir: "=${YARN_APP_MAPREDUCE_AM_STAGING_DIR}/history/done_intermediate"
    intermediate-user-done-dir:
      permissions: "770"
    always-scan-user-dir: "false"
    done-dir: "=${YARN_APP_MAPREDUCE_AM_STAGING_DIR}/history/done"
    cleaner:
      enable: "true"
      interval-ms: "86400000"
    max-age-ms: "604800000"
    client:
      thread-count: "10"
    datestring:
      cache:
        size: "200000"
    joblist:
      cache:
        size: "20000"
    loadedjob:
      tasks:
        max: "-1"
    loadedjobs:
      cache:
        size: "5"
    loadedtasks:
      cache:
        size: "NULL"
    move:
      interval-ms: "180000"
      thread-count: "3"
    store:
      class: "NULL"
    minicluster:
      fixed:
        ports: "false"
    admin:
      port: "10033"
      address: "?${MAPREDUCE_JOBHISTORY_HOSTNAME},${MAPREDUCE_JOBHISTORY_HOSTNAME}:${MAPREDUCE_JOBHISTORY_ADMIN_PORT},${HOSTNAME}:${MAPREDUCE_JOBHISTORY_ADMIN_PORT}"
      acl: "*"
    recovery:
      enable: "false"
      store:
        class: "org.apache.hadoop.mapreduce.v2.hs.HistoryServerFileSystemStateStoreService"
        fs:
          uri: "=${HADOOP_TMP_DIR}/mapred/history/recoverystore"
        leveldb:
          path: "=${HADOOP_TMP_DIR}/mapred/history/recoverystore"
    http:
      policy: "HTTP_ONLY"
    jobname:
      limit: "50"
    jhist:
      format: "binary"

  outputcommitter:
    factory:
      class: "NULL"
      scheme:
        s3a: "org.apache.hadoop.fs.s3a.commit.S3ACommitterFactory"

map:
  sort:
    class: "org.apache.hadoop.util.QuickSort"

mapred:
  child:
    java:
      opts: "NULL"
    env: "NULL"
  admin:
    user:
      env: "NULL"

yarn:
  app:
    mapreduce:
      task:
        container:
          log:
            backups: "0"
      am:
        log:
          level: "INFO"
        container:
          log:
            limit:
              kb: "0"
            backups: "0"
        staging-dir:
          "": "/tmp/hadoop-yarn/staging"
          erasurecoding:
            enabled: "false"
        env: "=HADOOP_MAPRED_HOME=${HADOOP_HOME}"
        admin:
          user:
            env: "NULL"
        command-opts: "-Xmx1024m"
        admin-command-opts: "NULL"
        job:
          task:
            listener:
              thread-count: "30"
          client:
            port-range: "NULL"
          committer:
            cancel-timeout: "60000"
            commit-window: "10000"
        webapp:
          port-range: "NULL"
        scheduler:
          heartbeat:
            interval-ms: "1000"
        resource:
          mb: "1536"
          cpu-vcores: "1"
        hard-kill-timeout-ms: "10000"
        containerlauncher:
          threadpool-initial-size: "10"
      shuffle:
        log:
          separate: "true"
          limit:
            kb: "0"
          backups: "0"
      client-am:
        ipc:
          max-retries: "3"
          max-retries-on-timeouts: "3"
      client:
        job:
          max-retries: "3"
          retry-interval: "2000"
        max-retries: "3"
